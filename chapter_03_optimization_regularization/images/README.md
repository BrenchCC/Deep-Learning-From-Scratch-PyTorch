# 实验效果可视化

## A.实验优化器效果可视化
![实验优化器效果可视化](optimizer_comparison.png)
这张图非常关键，它直接验证了你手写优化器代码的**正确性**，同时也直观展示了不同优化算法在复杂地形（Rosenbrock "香蕉函数"）下的行为差异。

我们可以从以下三个维度来解读这张图：

### 1. 核心验证：手写代码 vs 官方 API (Correctness)

这是本实验最重要的结论。

* **观察对象**：请看**红色虚线 (Manual SGD+Mom)** 和 **绿色点线 (PyTorch SGD+Mom)**。
* **现象**：这两条线在图中几乎**完全重合**，你很难把它们区分开。
* **结论**：这证明了你编写的 `StochasticGradientDescent` 类（包含动量实现）在数学逻辑和更新行为上与 PyTorch 官方的 `optim.SGD` **完全一致**。你的底层实现是正确的！

### 2. 优化器行为差异：SGD vs AdamW (Behavior)

Rosenbrock 函数之所以是经典的测试函数，是因为它有一个狭长且弯曲的山谷。

* **SGD with Momentum (红/绿线)**：
* **轨迹特征**：呈现出明显的**震荡和弯曲**。
* **原理解析**：SGD 沿着梯度最陡峭的方向走。进入山谷后，由于山谷两侧壁很陡（梯度大），SGD 会在两壁之间来回“撞击”。但得益于**动量 (Momentum)** 的存在，它积累了沿山谷走向的速度，虽然走得有些“踉跄”，但最终成功沿着弯曲的山谷底冲向了终点（五角星）。


* **AdamW (蓝线)**：
* **轨迹特征**：轨迹非常**平滑、笔直**，指向性很强，但在这张图中看起来似乎“走得比较慢”或步长较小。
* **原理解析**：
* **方向**：AdamW 通过二阶动量（梯度的平方）对每个参数的学习率进行了自适应调整。它能“感知”到哪个方向平坦、哪个方向陡峭，因此它不会像 SGD 那样剧烈震荡，而是走出了一条更平稳的路径。
* **步长 (为何这么短?)**：这是一个在大模型训练中常被忽视的高级现象。Rosenbrock 函数边缘的梯度非常大（陡峭）。Adam 的分母是梯度的均方根 ()。当梯度极大时，分母变得很大，导致**实际更新步长被自动缩小了**。相比之下，SGD 只是简单地乘上学习率（如果不做 Gradient Clipping），反而步子迈得更大。这体现了 Adam 的**安全性**——在地形剧烈变化时自动减速。





### 3. 地形与终点

* **背景等高线**：灰色越深代表地形越陡峭。我们可以看到一个明显的“U型”或“香蕉型”山谷。
* **目标**：黑色的五角星 (Global Min) 位于 。SGD 成功抵达，证明了动量机制克服局部震荡的能力。

### 总结

这张图完美达成了 Chapter 03 的目标：

1. **代码工程验证通过**：手写模块 = 官方模块。
2. **理论验证通过**：动量加速了 SGD 在峡谷中的收敛；AdamW 表现出了自适应的平滑特性（以及在大梯度下的自动步长缩放）。

## B.实验正则化效果可视化

![实验正则化效果可视化](regularization_comparison.png)

这张图非常完美地验证了我们 `exp_regularization.py` 代码中设计的实验逻辑，直观地展示了 **L1 (Lasso)**、**L2 (Ridge)** 和 **无正则化 (No Reg)** 三者在权

### 1. 实验设置回顾 (Ground Truth)

在代码中，我们设定了：

* **总特征数 (Features)**: 50 个。
* **有效特征 (Informative)**: 前 5 个（索引 0-4），真实权重设为 **1.0**。
* **噪声特征 (Noise)**: 后 45 个（索引 5-49），真实权重应为 **0.0**。
* **绿色虚线**: 分界线，左边是有效信号，右边全是噪声。

---

### 2. 图像深度解析

#### **区域 A：有效特征区 (Index 0 - 4)**

* **现象**: 灰色、蓝色、红色的柱子都接近 **1.0**。
* **代码映射**: `true_w[:5] = 1.0`。
* **解读**: 所有的模型（无论是否正则化）都成功捕捉到了这 5 个强信号。
* **微小差异**: 你会发现灰色（无正则）通常最高。红色（L1）和蓝色（L2）稍微低一点点。这是因为正则化项（Penalty）在某种程度上也在“惩罚”这些大权重，试图把它们也往下拉，但因为 Loss 中的 MSE 项占主导，所以它们依然保持在高位。



#### **区域 B：噪声特征区 (Index 5 - 49)**

这是最精彩的部分，展示了三种策略的差异：

**1. 灰色柱子 (No Reg / 无正则化)**

* **现象**: 在本该为 0 的区域，存在许多**杂乱的高低起伏**（例如 Index 48 附近有个明显的灰色柱子）。
* **原理**: `No Reg` 模型为了在训练集上降低 Loss，强行去拟合了我们在数据生成时加入的高斯噪声 (`np.random.normal`)。
* **后果**: **过拟合 (Overfitting)**。模型把噪声当成了信号。

**2. 蓝色柱子 (L2 / Ridge / Weight Decay)**

* **现象**: 相比灰色，蓝色柱子的**幅度明显变小**，看起来更“平滑”，但**几乎没有一个是真正的 0**。
* **原理**: 代码中我们使用了 `weight_decay`。
* 数学上，L2 的梯度是 。每次更新，权重都会按比例缩小（比如乘以 0.99）。
* 权重会无限趋近于 0，但像“芝诺的乌龟”一样，永远不会真正变成 0。


* **结论**: **平滑 (Smoothing)**。L2 抑制了噪声的影响，但保留了所有特征。

**3. 红色柱子 (L1 / Lasso)**

* **现象**: 在绿色虚线右侧，红色柱子**几乎完全消失**（归零）。
* **原理**: 代码中我们手动添加了 `l1_loss`。
* 数学上，L1 的梯度是 （常数）。
* 只要权重很小且对降低 MSE 贡献不大，L1 的常数惩罚就会像一把“激光刀”，直接把权重削减到 **0**。


* **结论**: **稀疏性 (Sparsity) 与 特征选择 (Feature Selection)**。L1 自动识别出后 45 个特征是无用的，并将其剔除。

---

### 3. 指导价值

这张图对我们做大模型（LLM）开发有极强的指导意义：

1. **稀疏激活 (Sparse Activation)**: 为什么现在的 MoE (Mixture of Experts) 或者某些 LLM 剪枝技术喜欢用 L1 范数？因为它可以产生稀疏的权重矩阵，从而实现**计算加速**（只计算非零项）。
2. **权重衰减 (AdamW)**: 在训练 Transformer 时，我们默认使用 `AdamW` (即解耦的 L2)。虽然它不会让权重变 0，但它能极好地控制模型复杂度，防止某些神经元权重过大导致数值溢出（fp16 training instability），这是**稳定性**的关键。
3. **可解释性**: 如果你在做一个金融风控模型或医疗诊断模型，L1 正则化可以告诉你“哪些特征是真正起作用的”（因为没用的都被置 0 了）。

**总结一句**：
灰色是**死记硬背**，蓝色是**中庸之道**（雨露均沾），红色是**断舍离**（去伪存真）。